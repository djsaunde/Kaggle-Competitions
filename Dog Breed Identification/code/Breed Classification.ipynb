{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "# Dog Breed Identification (Part I): Data Exploration\n",
    "\n",
    "We continue with our efforts on the [Dog Breed Identification Kaggle Competition](https://www.kaggle.com/c/dog-breed-identification). This time, we'll focus on building a deep __convolutional neural network__ to model the function between input images, $\\{ \\mathbf{X}^{(i)} \\}_{i = 1}^{N_{\\textrm{train}}}$, and corresponding labels, $\\{ y^{(i)} \\}_{i = 1}^{N_{\\textrm{train}}}$, where $N_{\\textrm{train}}$ = the number of _training data_ available. The labels $y^{(i)} \\in \\{ 1, ..., K \\}$ are proxy to the _object category_ of the input $\\mathbf{X}^{(i)}$; e.g., if some $\\mathbf{X}^{(i)}$ is an image containing an [Alaskan Malamute](https://en.wikipedia.org/wiki/Alaskan_Malamute), its label should be the integer which is mapped to the category \"Alaskan Malamute\". All possible object categories have a corresponding integer label.\n",
    "\n",
    "We also have access to a (smaller) _validation dataset_, which we periodically evaluate our network on to ensure we don't [overfit](https://en.wikipedia.org/wiki/Overfitting) the training data. We denote the validation data as $(\\{ \\mathbf{X}^{(i)}, y^{(i)} \\})_{i = 1}^{N_{\\textrm{validate}}}$, where $N_{\\textrm{validate}}$ = the number of _validation data_ available, and $N_{\\textrm{validate}} << N_{\\textrm{train}}$.\n",
    "\n",
    "A neural network can be expressed as a parametric function $f(\\mathbf{X}^{(i)}; \\theta)$; parametrized by a _parameter vector_ $\\theta$. The parameters correspond to the __learned weights__ on the connections between neurons of adjacent layers. The goal is to learn a setting of $\\theta$ which minimizes the differences between $\\sum_{i = 1}^{N_{\\textrm{train}}} \\left[ f(\\mathbf{X}^{(i)}; \\theta) - y^{(i)} \\right]$, while simultaneously choosing a reasonable setting of parameters that we expect to generalize well to new (e.g., test) data.\n",
    "\n",
    "For the Kaggle competition, we are given _test data_ $\\{ \\mathbf{X}^{(i)} \\}_{i = 1}^{N_{\\textrm{test}}}$, and we submit $\\{ f(\\mathbf{X}^{(i)}; \\hat{\\theta}^*) = \\hat{y}^{(i)} \\}_{i = 1}^{N_{\\textrm{test}}}$, where $\\hat{\\theta}^*$ is an estimate of optimal parameters given the particular neural network model. These __predictions__ (or __inferences__) will be compared with the [ground truth](https://en.wikipedia.org/wiki/Ground_truth) categorical labels, and we will be ranked according to the number of test data our model misclassified.\n",
    "\n",
    "At the time of writing, the best __error rate__ listed on the competition's leaderboard is 0.313% (accuracy of 100% - 0.313% = 99.687%). We don't expect to beat this, but obtaining a model with ~2-3% error rate is a realistic and challenging goal."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "## Imports / miscellany"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import torch\n",
    "import torch.optim\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "from torch.autograd import Variable\n",
    "from torch.utils.data import TensorDataset\n",
    "from torch.utils.data.dataloader import DataLoader\n",
    "\n",
    "%matplotlib inline\n",
    "\n",
    "data_path = os.path.join('..', 'data')\n",
    "\n",
    "# Are there CUDA-enabled GPU devices available? \n",
    "cuda = torch.cuda.device_count() > 0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "## Load pre-processed doggos\n",
    "\n",
    "We've already pre-processed the pupper image data. For now, we will simply load it into memory."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "# Load training (input, target) data.\n",
    "X_train = np.load(os.path.join(data_path, 'X_train.npy')).transpose((0, 3, 2, 1))\n",
    "y_train = np.load(os.path.join(data_path, 'y_train.npy'))\n",
    "y_train = np.array([ np.argmax(y_train[idx, :]) for idx in range(y_train.shape[0]) ])\n",
    "\n",
    "# Load validation (input, target) data.\n",
    "X_valid = np.load(os.path.join(data_path, 'X_valid.npy')).transpose((0, 3, 2, 1))\n",
    "y_valid = np.load(os.path.join(data_path, 'y_valid.npy'))\n",
    "y_valid = np.array([ np.argmax(y_valid[idx, :]) for idx in range(y_valid.shape[0]) ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training data shapes (X, y): ((8177, 3, 256, 256), (8177,))\n",
      "Validation data shapes (X, y): ((2045, 3, 256, 256), (2045,))\n"
     ]
    }
   ],
   "source": [
    "# Sanity check: print out training, validation data shapes\n",
    "print('Training data shapes (X, y):', (X_train.shape, y_train.shape))\n",
    "print('Validation data shapes (X, y):', (X_valid.shape, y_valid.shape))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "## Define PyTorch neural network model\n",
    "\n",
    "Here is where things get interesting. We will use the [PyTorch deep learning library](http://pytorch.org/) (which I highly recommend!) to create a convolutional neural network (CNN) to learn an approximate mapping between inputs $\\mathbf{X}$ and targets $y$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "class CNN(nn.Module):\n",
    "    '''\n",
    "    Defines the convolutional neural network model.\n",
    "    '''\n",
    "    def __init__(self, input_size, n_classes):\n",
    "        '''\n",
    "        Constructor for the CNN object.\n",
    "        \n",
    "        Arguments:\n",
    "            - input_size (int): The number of units in the input \"layer\".\n",
    "                Corresponds to the number of pixels in the input images.\n",
    "            - n_classes (int): The number of target categories in the data.\n",
    "        \n",
    "        Returns:\n",
    "            - Instantiated CNN object.\n",
    "        '''\n",
    "        super(CNN, self).__init__()\n",
    "        \n",
    "        # Convolutional layer portion of the network.\n",
    "        self.conv1 = nn.Conv2d(in_channels=3, out_channels=32, \\\n",
    "                                kernel_size=3, stride=1, padding=1)\n",
    "        self.mpool1 = nn.MaxPool2d(kernel_size=2)\n",
    "        self.conv2 = nn.Conv2d(in_channels=32, out_channels=32, \\\n",
    "                                kernel_size=3, stride=1, padding=1)\n",
    "        self.mpool2 = nn.MaxPool2d(kernel_size=2)\n",
    "        self.conv3 = nn.Conv2d(in_channels=32, out_channels=64, \\\n",
    "                                kernel_size=3, stride=1, padding=1)\n",
    "        self.mpool3 = nn.MaxPool2d(kernel_size=2)\n",
    "        \n",
    "        # Fully-connected layer portion of the network.\n",
    "        self.dense1 = nn.Linear(2048, 1024)\n",
    "        self.dense2 = nn.Linear(1024, 256)\n",
    "        self.dense3 = nn.Linear(256, n_classes)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        '''\n",
    "        Defines the forward pass of the network.\n",
    "        \n",
    "        Arguments:\n",
    "            - x (np.ndarray): A minibatch of images with\n",
    "                shape (M, D, H, W), with M = minibatch size.\n",
    "        \n",
    "        Returns:\n",
    "            - Activations of the final layer of the CNN. That is,\n",
    "                the representation of the input, learned by the network,\n",
    "                which is used to disentangle the correct object category.\n",
    "        '''\n",
    "        # First convolutional block (Conv2d -> MaxPool2d -> ReLU nonlinearity)\n",
    "        x_ = F.relu(self.mpool1(self.conv1(x)))\n",
    "        print(x_.size())\n",
    "        \n",
    "        # Second convolutional block (Conv2d -> MaxPool2d -> ReLU nonlinearity)\n",
    "        x_ = F.relu(self.mpool2(self.conv2(x_)))\n",
    "        print(x_.size())\n",
    "        \n",
    "        # Third convolutional block (Conv2d -> MaxPool2d -> ReLU nonlinearity)\n",
    "        x_ = F.relu(self.mpool3(self.conv3(x_)))\n",
    "        print(x_.size())\n",
    "        \n",
    "        # Flatten 3-dimensional output of last\n",
    "        # convolutional block to be 1-dimensional.\n",
    "        x_ = x_.view(-1, 1)  # Quirky!\n",
    "        print(x_.size())\n",
    "        \n",
    "        # First fully-connected block (Linear -> ReLU)\n",
    "        x_ = F.relu(self.dense1(x_))\n",
    "        print(x_.size())\n",
    "        \n",
    "        # Second fully-connected block (Linear -> ReLU)\n",
    "        x_ = F.relu(self.dense2(x_))\n",
    "        print(x_.size())\n",
    "        \n",
    "        # Third fully-connected block (Linear -> ReLU)\n",
    "        x_ = F.relu(self.dense3(x_))\n",
    "        print(x_.size())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "Now that we've defined the network, we can instantiate one and train it to identify the dog breeds in our image dataset. But first, we must choose network [hyperparameters](https://en.wikipedia.org/wiki/Hyperparameter_(machine_learning)) (parameters that are chosen prior to training which affect how the training operates, as opposed to parameters which are learned during network training). We also store some useful information in workspace-level variables, which can be changed once (here) to affect the rest of the notebook. Finally, we will use `torch.utils.data.DataLoader`s to simplify the presentation of data to the network."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data already cast to torch.Tensor data type.\n"
     ]
    }
   ],
   "source": [
    "# Hyperparameters\n",
    "n_epochs = 10  # No. of times to train on the entire training data.\n",
    "batch_size = 100  # No. of examples used in minibatch stochastic gradient descent (SGD).\n",
    "print_interval = 50  # No. of minibatch SGD iterations between each progress message.\n",
    "\n",
    "# Useful information\n",
    "input_size = (256, 256)  # As defined in \"Data Exploration.ipynb\".\n",
    "n_classes = 120  # No. of doggo breeds.\n",
    "\n",
    "# Cast training, validation data to `torch.Tensor`s.\n",
    "try:\n",
    "    X_train, y_train = torch.from_numpy(X_train).float(), torch.from_numpy(y_train)\n",
    "    X_valid, y_valid = torch.from_numpy(X_valid).float(), torch.from_numpy(y_valid)\n",
    "except RuntimeError:\n",
    "    print('Data already cast to torch.Tensor data type.')\n",
    "    \n",
    "# Data loaders\n",
    "train_loader = DataLoader(dataset=TensorDataset(X_train, y_train), batch_size=batch_size, shuffle=True)\n",
    "valid_loader = DataLoader(dataset=TensorDataset(X_valid, y_valid), batch_size=batch_size, shuffle=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "## Train the doggo recognizer\n",
    "\n",
    "We're ready to begin network training. We'll instantiate a network, define the criterion we aim to minimize ([Multiclass Log Loss](https://www.kaggle.com/wiki/MultiClassLogLoss)), define the optimization algorithm (we'll use [Adam](https://arxiv.org/abs/1412.6980), short for adaptive moments, a variant of SGD which dynamically updates individual parameter learning rates during training), and train the model for some number of epochs (the number of passes through the training data, given by `n_epochs`)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "# Instantiate CNN object.\n",
    "network = CNN(input_size=input_size, n_classes=n_classes)\n",
    "if cuda:\n",
    "    network.cuda()\n",
    "\n",
    "# Create loss / cost /objective function.\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "# Specify optimization routine.\n",
    "optimizer = torch.optim.Adam(network.parameters())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "Here is the training loop!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([100, 32, 128, 128])\n"
     ]
    }
   ],
   "source": [
    "for epoch in range(n_epochs):\n",
    "    # On each minibatch SGD iteration, we get `batch_size` samples from `X_train`.\n",
    "    for idx, (inputs, targets) in enumerate(train_loader):\n",
    "        # Convert `torch.Tensor`s to `Variable`s.\n",
    "        if cuda:\n",
    "            inputs = Variable(inputs.cuda())\n",
    "            targets = Variable(targets.cuda())\n",
    "        else:\n",
    "            inputs = Variable(inputs)\n",
    "            targets = Variable(targets)\n",
    "        \n",
    "        # Run forward, backward pass of network\n",
    "        optimizer.zero_grad()  # zeros out gradient buffer\n",
    "        predictions = network.forward(inputs)  # run forward pass of network to get predictions\n",
    "        loss = criterion(predictions, targets)  # calculate loss (fn. of predictions and true targets)\n",
    "        loss.backward()  # run backward pass (calculate grads. of loss w.r.t. network parameters)\n",
    "        \n",
    "        # Take optimization step (update network parameters in opposite direction of loss).\n",
    "        optimizer.step()\n",
    "        \n",
    "        if i % print_interval == 0:\n",
    "            print('Epoch [%d / %d], Iteration [%d / %d], Loss: %.4f' % \\\n",
    "                      (epoch + 1, n_epochs, idx + 1, ))"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [conda env:py36]",
   "language": "python",
   "name": "conda-env-py36-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
