{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dog Breed Identification (Part II): Breed Classification\n",
    "\n",
    "We continue with our efforts on the [Dog Breed Identification Kaggle Competition](https://www.kaggle.com/c/dog-breed-identification). This time, we'll focus on building a deep __convolutional neural network__ to model the function between input images, $\\{ \\mathbf{X}^{(i)} \\}_{i = 1}^{N_{\\textrm{train}}}$, and corresponding labels, $\\{ y^{(i)} \\}_{i = 1}^{N_{\\textrm{train}}}$, where $N_{\\textrm{train}}$ = the number of _training data_ available. The labels $y^{(i)} \\in \\{ 1, ..., K \\}$ are proxy to the _object category_ of the input $\\mathbf{X}^{(i)}$; e.g., if some $\\mathbf{X}^{(i)}$ is an image containing an [Alaskan Malamute](https://en.wikipedia.org/wiki/Alaskan_Malamute), its label should be the integer which is mapped to the category \"Alaskan Malamute\". All possible object categories have a corresponding integer label.\n",
    "\n",
    "We also have access to a (smaller) _validation dataset_, which we periodically evaluate our network on to ensure we don't [overfit](https://en.wikipedia.org/wiki/Overfitting) the training data. We denote the validation data as $(\\{ \\mathbf{X}^{(i)}, y^{(i)} \\})_{i = 1}^{N_{\\textrm{validate}}}$, where $N_{\\textrm{validate}}$ = the number of _validation data_ available, and $N_{\\textrm{validate}} << N_{\\textrm{train}}$.\n",
    "\n",
    "A neural network can be expressed as a parametric function $f(\\mathbf{X}^{(i)}; \\theta)$; parametrized by a _parameter vector_ $\\theta$. The parameters correspond to the __learned weights__ on the connections between neurons of adjacent layers. The goal is to learn a setting of $\\theta$ which minimizes the differences between $\\sum_{i = 1}^{N_{\\textrm{train}}} \\left[ f(\\mathbf{X}^{(i)}; \\theta) - y^{(i)} \\right]$, while simultaneously choosing a reasonable setting of parameters that we expect to generalize well to new (e.g., test) data.\n",
    "\n",
    "For the Kaggle competition, we are given _test data_ $\\{ \\mathbf{X}^{(i)} \\}_{i = 1}^{N_{\\textrm{test}}}$, and we submit $\\{ f(\\mathbf{X}^{(i)}; \\hat{\\theta}^*) = \\hat{y}^{(i)} \\}_{i = 1}^{N_{\\textrm{test}}}$, where $\\hat{\\theta}^*$ is an estimate of optimal parameters given the particular neural network model. These __predictions__ (or __inferences__) will be compared with the [ground truth](https://en.wikipedia.org/wiki/Ground_truth) categorical labels, and we will be ranked according to the number of test data our model misclassified.\n",
    "\n",
    "At the time of writing, the best __error rate__ listed on the competition's leaderboard is 0.313% (accuracy of 100% - 0.313% = 99.687%). We don't expect to beat this, but obtaining a model with ~2-3% error rate is a realistic and challenging goal."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports / miscellany"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import torch\n",
    "import torch.optim\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "from torch.autograd import Variable\n",
    "from torch.utils.data import TensorDataset\n",
    "from torch.utils.data.dataloader import DataLoader\n",
    "\n",
    "%matplotlib inline\n",
    "\n",
    "train_path = os.path.join('..', 'data', 'processed_train')\n",
    "valid_path = os.path.join('..', 'data', 'processed_valid')\n",
    "test_path = os.path.join('..', 'data', 'processed_test')\n",
    "\n",
    "# Are there CUDA-enabled GPU devices available? \n",
    "cuda = torch.cuda.device_count() > 0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load pre-processed doggos\n",
    "\n",
    "We've already pre-processed the pupper image data. For now, we will simply load it into memory."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ListDataset(torch.utils.data.Dataset):\n",
    "    '''\n",
    "    Custom torch.utils.Dataset used for reading\n",
    "    in a list of data files from disk.\n",
    "    '''\n",
    "    def __init__(self, data_path):\n",
    "        self.data_files = os.listdir(data_path)\n",
    "        sorted(self.data_files)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return load_file(self.data_files[idx])\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data_files)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load training (input, target) data.\n",
    "train_data = ListDataset(train_path)\n",
    "\n",
    "# Load validation (input, target) data.\n",
    "valid_data = ListDataset(valid_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No. of training data: 8177\n",
      "No. of validation data: 2045\n"
     ]
    }
   ],
   "source": [
    "# Sanity check: print out training, validation data shapes\n",
    "print('No. of training data:', len(train_data))\n",
    "print('No. of validation data:', len(valid_data))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define PyTorch neural network model\n",
    "\n",
    "Here is where things get interesting. We will use the [PyTorch deep learning library](http://pytorch.org/) (which I highly recommend!) to create a convolutional neural network (CNN) to learn an approximate mapping between inputs $\\mathbf{X}$ and targets $y$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class CNN(nn.Module):\n",
    "    '''\n",
    "    Defines the convolutional neural network model.\n",
    "    '''\n",
    "    def __init__(self, input_size, n_classes):\n",
    "        '''\n",
    "        Constructor for the CNN object.\n",
    "        \n",
    "        Arguments:\n",
    "            - input_size (int): The number of units in the input \"layer\".\n",
    "                Corresponds to the number of pixels in the input images.\n",
    "            - n_classes (int): The number of target categories in the data.\n",
    "        \n",
    "        Returns:\n",
    "            - Instantiated CNN object.\n",
    "        '''\n",
    "        super(CNN, self).__init__()\n",
    "        \n",
    "        # Convolutional layer portion of the network.\n",
    "        self.convolutional = nn.Sequential(\n",
    "            nn.Conv2d(in_channels=3, out_channels=16, \\\n",
    "                    kernel_size=5, stride=1, padding=2),\n",
    "            nn.BatchNorm2d(16),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(kernel_size=2),\n",
    "            nn.Conv2d(in_channels=16, out_channels=16, \\\n",
    "                    kernel_size=5, stride=1, padding=2),\n",
    "            nn.BatchNorm2d(16),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(kernel_size=2),\n",
    "            nn.Conv2d(in_channels=16, out_channels=32, \\\n",
    "                    kernel_size=5, stride=1, padding=2),\n",
    "            nn.BatchNorm2d(32),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(kernel_size=2),\n",
    "            nn.Conv2d(in_channels=32, out_channels=32, \\\n",
    "                    kernel_size=5, stride=1, padding=2),\n",
    "            nn.BatchNorm2d(32),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(kernel_size=2),\n",
    "            nn.Conv2d(in_channels=32, out_channels=32, \\\n",
    "                    kernel_size=5, stride=1, padding=2),\n",
    "            nn.BatchNorm2d(32),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(kernel_size=2),\n",
    "            nn.Conv2d(in_channels=32, out_channels=32, \\\n",
    "                    kernel_size=5, stride=1, padding=2),\n",
    "            nn.BatchNorm2d(32),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(kernel_size=2)\n",
    "        )\n",
    "        \n",
    "        # Fully-connected layer portion of the network.\n",
    "        self.dense = nn.Sequential(\n",
    "            nn.Dropout(p=0.5),\n",
    "            nn.Linear(512, 256),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(256, n_classes)\n",
    "        )\n",
    "    \n",
    "    def forward(self, x):\n",
    "        '''\n",
    "        Defines the forward pass of the network.\n",
    "        \n",
    "        Arguments:\n",
    "            - x (np.ndarray): A minibatch of images with\n",
    "                shape (M, D, H, W), with M = minibatch size.\n",
    "        \n",
    "        Returns:\n",
    "            - Activations of the final layer of the CNN. That is,\n",
    "                the representation of the input, learned by the network,\n",
    "                which is used to disentangle the correct object category.\n",
    "        '''\n",
    "        # Get features computed by convolutional portion of the network.\n",
    "        conv_features = self.convolutional(x)\n",
    "        \n",
    "        # Flatten these features from 4D (batch_size, D, H, W)\n",
    "        # tensor to 2D (batch_size, D * H * W) tensor.\n",
    "        flat_features = conv_features.view(conv_features.size(0), -1)\n",
    "                \n",
    "        # Get prediction computed by the fully-connected portion of the network.\n",
    "        predictions = self.dense(flat_features)\n",
    "        \n",
    "        # Return the processed input data as the predicted target values.\n",
    "        return predictions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we've defined the network, we can instantiate one and train it to identify the dog breeds in our image dataset. But first, we must choose network [hyperparameters](https://en.wikipedia.org/wiki/Hyperparameter_(machine_learning)) (parameters that are chosen prior to training which affect how the training operates, as opposed to parameters which are learned during network training). We also store some useful information in workspace-level variables, which can be changed once (here) to affect the rest of the notebook. Finally, we will use `torch.utils.data.DataLoader`s to simplify the presentation of data to the network."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hyperparameters\n",
    "n_epochs = 50  # No. of times to train on the entire training data.\n",
    "batch_size = 100  # No. of examples used in minibatch stochastic gradient descent (SGD).\n",
    "print_interval = 10  # No. of minibatch SGD iterations between each progress message.\n",
    "\n",
    "# Useful information\n",
    "input_size = (256, 256)  # As defined in \"Data Exploration.ipynb\".\n",
    "n_classes = 120  # No. of doggo breeds.\n",
    "\n",
    "# Data loaders\n",
    "train_loader = DataLoader(dataset=train_data, batch_size=batch_size, shuffle=True, num_workers=8)\n",
    "valid_loader = DataLoader(dataset=valid_data, batch_size=batch_size, shuffle=True, num_workers=8)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train the doggo recognizer\n",
    "\n",
    "We're ready to begin network training. We'll instantiate a network, define the criterion we aim to minimize ([Multiclass Log Loss](https://www.kaggle.com/wiki/MultiClassLogLoss)), define the optimization algorithm (we'll use [Adam](https://arxiv.org/abs/1412.6980), short for adaptive moments, a variant of SGD which dynamically updates individual parameter learning rates during training), and train the model for some number of epochs (the number of passes through the training data, given by `n_epochs`)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Instantiate CNN object.\n",
    "network = CNN(input_size=input_size, n_classes=n_classes)\n",
    "if cuda:\n",
    "    network.cuda()\n",
    "\n",
    "# Create loss / cost /objective function.\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "# Specify optimization routine.\n",
    "optimizer = torch.optim.Adam(network.parameters(), weight_decay=1e-2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here is the training loop!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "ename": "NotImplementedError",
     "evalue": "Traceback (most recent call last):\n  File \"/home/djsaunde/anaconda2/envs/py36/lib/python3.6/site-packages/torch/utils/data/dataloader.py\", line 40, in _worker_loop\n    samples = collate_fn([dataset[i] for i in batch_indices])\n  File \"/home/djsaunde/anaconda2/envs/py36/lib/python3.6/site-packages/torch/utils/data/dataloader.py\", line 40, in <listcomp>\n    samples = collate_fn([dataset[i] for i in batch_indices])\n  File \"/home/djsaunde/anaconda2/envs/py36/lib/python3.6/site-packages/torch/utils/data/dataset.py\", line 13, in __getitem__\n    raise NotImplementedError\nNotImplementedError\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNotImplementedError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-22-2ede32fa6eb7>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m     \u001b[0;31m# On each minibatch SGD iteration, we get `batch_size` samples from `X_train`.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m     \u001b[0;32mfor\u001b[0m \u001b[0midx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtargets\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_loader\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      6\u001b[0m         \u001b[0;31m# Convert `torch.Tensor`s to `Variable`s.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mcuda\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda2/envs/py36/lib/python3.6/site-packages/torch/utils/data/dataloader.py\u001b[0m in \u001b[0;36m__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    199\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreorder_dict\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0midx\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbatch\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    200\u001b[0m                 \u001b[0;32mcontinue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 201\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_process_next_batch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    202\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    203\u001b[0m     \u001b[0mnext\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m__next__\u001b[0m  \u001b[0;31m# Python 2 compatibility\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda2/envs/py36/lib/python3.6/site-packages/torch/utils/data/dataloader.py\u001b[0m in \u001b[0;36m_process_next_batch\u001b[0;34m(self, batch)\u001b[0m\n\u001b[1;32m    219\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_put_indices\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    220\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mExceptionWrapper\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 221\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0mbatch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexc_type\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexc_msg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    222\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mbatch\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    223\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNotImplementedError\u001b[0m: Traceback (most recent call last):\n  File \"/home/djsaunde/anaconda2/envs/py36/lib/python3.6/site-packages/torch/utils/data/dataloader.py\", line 40, in _worker_loop\n    samples = collate_fn([dataset[i] for i in batch_indices])\n  File \"/home/djsaunde/anaconda2/envs/py36/lib/python3.6/site-packages/torch/utils/data/dataloader.py\", line 40, in <listcomp>\n    samples = collate_fn([dataset[i] for i in batch_indices])\n  File \"/home/djsaunde/anaconda2/envs/py36/lib/python3.6/site-packages/torch/utils/data/dataset.py\", line 13, in __getitem__\n    raise NotImplementedError\nNotImplementedError\n"
     ]
    }
   ],
   "source": [
    "for epoch in range(n_epochs):\n",
    "    train_correct, train_total = 0, 0\n",
    "    \n",
    "    # On each minibatch SGD iteration, we get `batch_size` samples from `X_train`.\n",
    "    for idx, (inputs, targets) in enumerate(train_loader):\n",
    "        # Convert `torch.Tensor`s to `Variable`s.\n",
    "        if cuda:\n",
    "            inputs = Variable(inputs.cuda())\n",
    "            targets = Variable(targets.cuda())\n",
    "        else:\n",
    "            inputs = Variable(inputs)\n",
    "            targets = Variable(targets)\n",
    "        \n",
    "        # Run forward, backward pass of network.\n",
    "        # Zero out gradient buffer.\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        # Run forward pass of network to get predictions.\n",
    "        predictions = network.forward(inputs)\n",
    "        \n",
    "        # Get integer predictions by selecting the maximal output activation.\n",
    "        _, predicted = torch.max(predictions.data, 1)\n",
    "        \n",
    "        # Add correct classifications to a running sum.\n",
    "        train_correct += (predicted.cpu() == targets.data.cpu()).sum()\n",
    "        \n",
    "        # Add number of items in the minibatch to running sum.\n",
    "        train_total += targets.size(0)\n",
    "        \n",
    "        # Calculate loss (non-negative function of predictions and true targets).\n",
    "        loss = criterion(predictions, targets)\n",
    "        \n",
    "        # Run backward pass (calculate gradient of loss w.r.t. network parameters).\n",
    "        loss.backward()\n",
    "        \n",
    "        # Take optimization step (update network parameters in opposite direction of loss).\n",
    "        optimizer.step()\n",
    "        \n",
    "        if idx % print_interval == 0:\n",
    "            print('Epoch [%d / %d], Iteration [%d / %d], Loss: %.4f' % (epoch + 1, \\\n",
    "                                n_epochs, idx + 1, len(train_loader), loss.data[0]))\n",
    "    \n",
    "    valid_correct, valid_total = 0, 0\n",
    "    \n",
    "    # Calculate the accuracy of the network on the\n",
    "    # validation data at the end of each epoch.\n",
    "    for idx, (inputs, targets) in enumerate(valid_loader):\n",
    "        # Convert `torch.Tensor`s to `Variable`s.\n",
    "        if cuda:\n",
    "            inputs = Variable(inputs.cuda())\n",
    "        else:\n",
    "            inputs = Variable(inputs)\n",
    "        \n",
    "        predictions = network.forward(inputs)\n",
    "        _, predicted = torch.max(predictions.data, 1)\n",
    "        valid_correct += (predicted.cpu() == targets).sum()\n",
    "        \n",
    "        valid_total += targets.size(0)\n",
    "        \n",
    "    print()\n",
    "    print('Training accuracy: %.4f' % (100 * train_correct / train_total))\n",
    "    print('Validation accuracy: %.4f' % (100 * valid_correct / valid_total))\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
